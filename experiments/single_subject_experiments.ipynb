{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cf0eaa-cdea-4c0b-8069-34726ed03929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Result_1-_Data_with_Phase1_RLS.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os, glob, random, time, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa47b85-4508-4d4f-ba56-c579617dba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 RLS attention model import\n",
    "from models_rls_phase1 import Phase1RLSModel\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b24659-800e-470c-8347-41c76d4f76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class unchanged\n",
    "class SEEDVII_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir: str = \".\", modality: str = 'multimodal', \n",
    "                 subset_ratio: float = 0.01):\n",
    "        self.data_dir = data_dir\n",
    "        self.modality = modality\n",
    "        self.subset_ratio = subset_ratio\n",
    "        \n",
    "        # Dataset specifications\n",
    "        self.num_classes = 7\n",
    "        self.num_subjects = 20\n",
    "        self.eeg_feature_dim = 310\n",
    "        self.eye_feature_dim = 33\n",
    "        \n",
    "        # Load and process data\n",
    "        self._load_data()\n",
    "        if self.subset_ratio < 1.0:\n",
    "            self._create_subset()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        \"\"\"Loading EEG and eye movement features here.\"\"\"\n",
    "        print(f\"Loading SEED-VII dataset from {self.data_dir}\")\n",
    "        \n",
    "        eeg_data, eye_data = [], []\n",
    "        emotion_labels, subject_labels = [], []\n",
    "        \n",
    "        # Get feature files\n",
    "        eeg_dir = os.path.join(self.data_dir, 'EEG_features')\n",
    "        eye_dir = os.path.join(self.data_dir, 'EYE_features')\n",
    "        \n",
    "        eeg_files = sorted(glob.glob(os.path.join(eeg_dir, '*.mat')))\n",
    "        eye_files = sorted(glob.glob(os.path.join(eye_dir, '*.mat')))\n",
    "        \n",
    "        print(f\"Found {len(eeg_files)} EEG files and {len(eye_files)} eye files\")\n",
    "        \n",
    "        # Simple emotion mapping (based on paper structure)\n",
    "        emotion_map = self._get_emotion_mapping()\n",
    "        \n",
    "        # Process subjects\n",
    "        for subject_idx, eeg_file in enumerate(eeg_files[:self.num_subjects]):\n",
    "            subject_name = os.path.basename(eeg_file).replace('.mat', '')\n",
    "            print(f\"Loading subject {subject_idx + 1}: {subject_name}\")\n",
    "            \n",
    "            # Load files\n",
    "            try:\n",
    "                eeg_mat = sio.loadmat(eeg_file)\n",
    "                eye_file = self._find_matching_eye_file(subject_name, eye_files)\n",
    "                if eye_file:\n",
    "                    eye_mat = sio.loadmat(eye_file)\n",
    "                else:\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading files for {subject_name}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Process videos\n",
    "            for video_id in range(1, 81):\n",
    "                eeg_features, eye_features = self._extract_features(\n",
    "                    eeg_mat, eye_mat, video_id)\n",
    "                \n",
    "                if eeg_features is not None and eye_features is not None:\n",
    "                    min_windows = min(len(eeg_features), len(eye_features))\n",
    "                    if min_windows > 0:\n",
    "                        eeg_data.append(eeg_features[:min_windows])\n",
    "                        eye_data.append(eye_features[:min_windows])\n",
    "                        \n",
    "                        emotion_label = emotion_map.get(video_id, 6)\n",
    "                        emotion_labels.extend([emotion_label] * min_windows)\n",
    "                        subject_labels.extend([subject_idx] * min_windows)\n",
    "        \n",
    "        # Convert to arrays\n",
    "        self.eeg_features = np.vstack(eeg_data)\n",
    "        self.eye_features = np.vstack(eye_data)\n",
    "        self.emotion_labels = np.array(emotion_labels)\n",
    "        self.subject_labels = np.array(subject_labels)\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(self.emotion_labels)} samples\")\n",
    "        print(f\"EEG shape: {self.eeg_features.shape}, Eye shape: {self.eye_features.shape}\")\n",
    "    \n",
    "    def _get_emotion_mapping(self):\n",
    "        \"\"\"Here I tried to replicate a simple emotion mapping from the MAET paper.\"\"\"\n",
    "        emotion_map = {}\n",
    "        # Simplified mapping - 4 videos per emotion per session\n",
    "        for session in range(4):\n",
    "            emotions = [0, 6, 3, 1, 5, 2, 4][0:7] if session % 2 == 0 else [5, 1, 2, 6, 0, 4, 3]\n",
    "            for i, emotion in enumerate(emotions):\n",
    "                for video in range(4):\n",
    "                    video_id = session * 20 + i * 4 + video + 1\n",
    "                    if video_id <= 80:\n",
    "                        emotion_map[video_id] = emotion % 7\n",
    "        return emotion_map\n",
    "    \n",
    "    def _find_matching_eye_file(self, subject_name, eye_files):\n",
    "        \"\"\"This code is to find the matching eye file for the subjects.\"\"\"\n",
    "        for eye_file in eye_files:\n",
    "            if subject_name in os.path.basename(eye_file):\n",
    "                return eye_file\n",
    "        return None\n",
    "    \n",
    "    def _extract_features(self, eeg_mat, eye_mat, video_id):\n",
    "        video_key = str(video_id)\n",
    "        \n",
    "        # Try different key formats for EEG\n",
    "        eeg_features = None\n",
    "        for key in [f'de_LDS_{video_id}', f'de_{video_id}', video_key]:\n",
    "            if key in eeg_mat:\n",
    "                eeg_features = eeg_mat[key]\n",
    "                break\n",
    "        \n",
    "        # Try different key formats for Eye\n",
    "        eye_features = None\n",
    "        for key in [video_key, str(video_id)]:\n",
    "            if key in eye_mat:\n",
    "                eye_features = eye_mat[key]\n",
    "                break\n",
    "        \n",
    "        # Process EEG features\n",
    "        if eeg_features is not None:\n",
    "            if eeg_features.ndim == 3:\n",
    "                eeg_features = eeg_features.reshape(eeg_features.shape[0], -1)\n",
    "            if eeg_features.shape[1] != self.eeg_feature_dim:\n",
    "                eeg_features = None\n",
    "        \n",
    "        # Process eye features\n",
    "        if eye_features is not None and eye_features.shape[1] != self.eye_feature_dim:\n",
    "            eye_features = None\n",
    "        \n",
    "        return eeg_features, eye_features\n",
    "    \n",
    "    def _create_subset(self):\n",
    "        n_samples = len(self.emotion_labels)\n",
    "        subset_size = max(1, int(n_samples * self.subset_ratio))\n",
    "        \n",
    "        try:\n",
    "            indices = np.arange(n_samples)\n",
    "            subset_indices, _ = train_test_split(\n",
    "                indices, train_size=subset_size, stratify=self.emotion_labels, random_state=42)\n",
    "        except:\n",
    "            subset_indices = np.random.choice(n_samples, subset_size, replace=False)\n",
    "        \n",
    "        self.eeg_features = self.eeg_features[subset_indices]\n",
    "        self.eye_features = self.eye_features[subset_indices]\n",
    "        self.emotion_labels = self.emotion_labels[subset_indices]\n",
    "        self.subject_labels = self.subject_labels[subset_indices]\n",
    "        \n",
    "        print(f\"Created {self.subset_ratio*100:.1f}% subset: {len(self.emotion_labels)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.emotion_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {}\n",
    "        \n",
    "        if self.modality in ['eeg', 'multimodal']:\n",
    "            sample['eeg'] = torch.FloatTensor(self.eeg_features[idx])\n",
    "        if self.modality in ['eye', 'multimodal']:\n",
    "            sample['eye'] = torch.FloatTensor(self.eye_features[idx])\n",
    "        \n",
    "        sample['label'] = torch.LongTensor([self.emotion_labels[idx]])[0]\n",
    "        sample['subject'] = torch.LongTensor([self.subject_labels[idx]])[0]\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e09118-c72b-4c35-ab54-221189763ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe forward stripped down for single-model\n",
    "def safe_forward(model, eeg=None):\n",
    "    return model(eeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15d43424-d6a6-49a6-a635-df6b00ca5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_rls(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0., 0, 0\n",
    "    for batch in dataloader:\n",
    "        eeg = batch['eeg'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(eeg)               # (B, 1, 7)\n",
    "        logits = logits.squeeze(1)           # (B, 7)\n",
    "        # DEBUG: print shapes to diagnose mismatch\n",
    "        #print(f\"LOGITS SHAPE: {logits.shape}, LABELS SHAPE: {labels.shape}\")\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), 100. * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3740bb-2667-4a1f-9bfd-7279062a7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rls(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0., 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            eeg = batch['eeg'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            logits, _ = model(eeg)\n",
    "            logits = logits.squeeze(1)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    acc = 100.*correct/total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')*100\n",
    "    return total_loss/len(dataloader), acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06499c45-903d-432e-b337-9ec524cc287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject-dependent experiment using Phase1RLSModel\n",
    "def subject_dependent_experiment_rls(data_dir=\".\", subset_ratio=0.01):\n",
    "    print(\"\\n=== SUBJECT-DEPENDENT (RLS) ===\")\n",
    "    dataset = SEEDVII_Dataset(data_dir, modality='eeg', subset_ratio=subset_ratio)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    subjects = np.unique(dataset.subject_labels)\n",
    "    results = []\n",
    "    for subj in subjects[:5]:\n",
    "        idxs = np.where(dataset.subject_labels==subj)[0]\n",
    "        if len(idxs)<8: continue\n",
    "        tr, vl = train_test_split(idxs, test_size=0.3, random_state=42)\n",
    "        tr_loader = DataLoader(Subset(dataset, tr), batch_size=16, shuffle=True)\n",
    "        vl_loader = DataLoader(Subset(dataset, vl), batch_size=16, shuffle=False)\n",
    "        # Instantiate Phase1RLSModel\n",
    "        model = Phase1RLSModel(\n",
    "            n_channels=dataset.eeg_feature_dim,\n",
    "            d_model=256, n_classes=dataset.num_classes\n",
    "        ).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        best_acc = 0.\n",
    "        for epoch in range(20):\n",
    "            _, train_acc = train_epoch_rls(model, tr_loader, opt, crit, device)\n",
    "            _, val_acc, _ = evaluate_rls(model, vl_loader, crit, device)\n",
    "            best_acc = max(best_acc, val_acc)\n",
    "            if epoch%5==0:\n",
    "                print(f\" Subj {subj} Epoch {epoch}: Train {train_acc:.1f}%, Val {val_acc:.1f}%\")\n",
    "        print(f\" Subj {subj} best Val Acc: {best_acc:.2f}%\")\n",
    "        results.append(best_acc)\n",
    "    avg, std = np.mean(results), np.std(results)\n",
    "    print(f\" Subject-dependent RLS results: {avg:.2f}% ± {std:.2f}%\")\n",
    "    return avg, std\n",
    "\n",
    "# Cross-subject experiment using Phase1RLSModel\n",
    "def cross_subject_experiment_rls(data_dir=\".\", subset_ratio=0.01):\n",
    "    print(\"\\n=== CROSS-SUBJECT (RLS) ===\")\n",
    "    dataset = SEEDVII_Dataset(data_dir, modality='eeg', subset_ratio=subset_ratio)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    subjects = np.unique(dataset.subject_labels)\n",
    "    results = []\n",
    "    for test_subj in subjects[:3]:\n",
    "        tr_idxs = np.where(dataset.subject_labels!=test_subj)[0]\n",
    "        te_idxs = np.where(dataset.subject_labels==test_subj)[0]\n",
    "        if len(te_idxs)<5: continue\n",
    "        tr_loader = DataLoader(Subset(dataset, tr_idxs), batch_size=16, shuffle=True)\n",
    "        te_loader = DataLoader(Subset(dataset, te_idxs), batch_size=16, shuffle=False)\n",
    "        model = Phase1RLSModel(\n",
    "            n_channels=dataset.eeg_feature_dim,\n",
    "            d_model=256, n_classes=dataset.num_classes\n",
    "        ).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        for epoch in range(30):\n",
    "            train_epoch_rls(model, tr_loader, opt, crit, device)\n",
    "            if epoch%10==0:\n",
    "                loss, acc, _ = evaluate_rls(model, te_loader, crit, device)\n",
    "                print(f\" TestSubj {test_subj} Epoch {epoch}: Acc {acc:.1f}%\")\n",
    "        _, test_acc, _ = evaluate_rls(model, te_loader, crit, device)\n",
    "        print(f\" TestSubj {test_subj} final Acc: {test_acc:.2f}%\")\n",
    "        results.append(test_acc)\n",
    "    if results:\n",
    "        avg, std = np.mean(results), np.std(results)\n",
    "        print(f\" Cross-subject RLS results: {avg:.2f}% ± {std:.2f}%\")\n",
    "        return avg, std\n",
    "    return 0., 0.\n",
    "\n",
    "def main():\n",
    "    print(\"=== PHASE1 RLS ON SEED-VII ===\")\n",
    "    DATA_DIR = \".\"\n",
    "    SUBSET_RATIO = 0.01\n",
    "    results = {}\n",
    "    # Dataset test\n",
    "    _ = SEEDVII_Dataset(DATA_DIR, 'eeg', SUBSET_RATIO)\n",
    "    # Run experiments\n",
    "    results['subj_dep'] = subject_dependent_experiment_rls(DATA_DIR, SUBSET_RATIO)\n",
    "    results['cross_subj'] = cross_subject_experiment_rls(DATA_DIR, SUBSET_RATIO)\n",
    "    print(\"\\nFINAL RESULTS:\")\n",
    "    for k,(a,s) in results.items():\n",
    "        print(f\" {k}: {a:.2f}% ± {s:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a94283e-56d8-47b1-aeac-53f64e8550df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE1 RLS ON SEED-VII ===\n",
      "Loading SEED-VII dataset from .\n",
      "Found 20 EEG files and 20 eye files\n",
      "Loading subject 1: 1\n",
      "Loading subject 2: 10\n",
      "Loading subject 3: 11\n",
      "Loading subject 4: 12\n",
      "Loading subject 5: 13\n",
      "Loading subject 6: 14\n",
      "Loading subject 7: 15\n",
      "Loading subject 8: 16\n",
      "Loading subject 9: 17\n",
      "Loading subject 10: 18\n",
      "Loading subject 11: 19\n",
      "Loading subject 12: 2\n",
      "Loading subject 13: 20\n",
      "Loading subject 14: 3\n",
      "Loading subject 15: 4\n",
      "Loading subject 16: 5\n",
      "Loading subject 17: 6\n",
      "Loading subject 18: 7\n",
      "Loading subject 19: 8\n",
      "Loading subject 20: 9\n",
      "Dataset loaded: 69742 samples\n",
      "EEG shape: (69742, 310), Eye shape: (69742, 33)\n",
      "Created 1.0% subset: 697 samples\n",
      "\n",
      "=== SUBJECT-DEPENDENT (RLS) ===\n",
      "Loading SEED-VII dataset from .\n",
      "Found 20 EEG files and 20 eye files\n",
      "Loading subject 1: 1\n",
      "Loading subject 2: 10\n",
      "Loading subject 3: 11\n",
      "Loading subject 4: 12\n",
      "Loading subject 5: 13\n",
      "Loading subject 6: 14\n",
      "Loading subject 7: 15\n",
      "Loading subject 8: 16\n",
      "Loading subject 9: 17\n",
      "Loading subject 10: 18\n",
      "Loading subject 11: 19\n",
      "Loading subject 12: 2\n",
      "Loading subject 13: 20\n",
      "Loading subject 14: 3\n",
      "Loading subject 15: 4\n",
      "Loading subject 16: 5\n",
      "Loading subject 17: 6\n",
      "Loading subject 18: 7\n",
      "Loading subject 19: 8\n",
      "Loading subject 20: 9\n",
      "Dataset loaded: 69742 samples\n",
      "EEG shape: (69742, 310), Eye shape: (69742, 33)\n",
      "Created 1.0% subset: 697 samples\n",
      " Subj 0 Epoch 0: Train 4.3%, Val 30.0%\n",
      " Subj 0 Epoch 5: Train 17.4%, Val 30.0%\n",
      " Subj 0 Epoch 10: Train 8.7%, Val 20.0%\n",
      " Subj 0 Epoch 15: Train 17.4%, Val 10.0%\n",
      " Subj 0 best Val Acc: 30.00%\n",
      " Subj 1 Epoch 0: Train 13.6%, Val 20.0%\n",
      " Subj 1 Epoch 5: Train 31.8%, Val 20.0%\n",
      " Subj 1 Epoch 10: Train 9.1%, Val 0.0%\n",
      " Subj 1 Epoch 15: Train 13.6%, Val 20.0%\n",
      " Subj 1 best Val Acc: 20.00%\n",
      " Subj 2 Epoch 0: Train 20.0%, Val 23.1%\n",
      " Subj 2 Epoch 5: Train 20.0%, Val 23.1%\n",
      " Subj 2 Epoch 10: Train 30.0%, Val 23.1%\n",
      " Subj 2 Epoch 15: Train 33.3%, Val 23.1%\n",
      " Subj 2 best Val Acc: 23.08%\n",
      " Subj 3 Epoch 0: Train 36.0%, Val 36.4%\n",
      " Subj 3 Epoch 5: Train 32.0%, Val 36.4%\n",
      " Subj 3 Epoch 10: Train 60.0%, Val 36.4%\n",
      " Subj 3 Epoch 15: Train 56.0%, Val 36.4%\n",
      " Subj 3 best Val Acc: 36.36%\n",
      " Subj 4 Epoch 0: Train 29.6%, Val 33.3%\n",
      " Subj 4 Epoch 5: Train 14.8%, Val 16.7%\n",
      " Subj 4 Epoch 10: Train 11.1%, Val 33.3%\n",
      " Subj 4 Epoch 15: Train 14.8%, Val 16.7%\n",
      " Subj 4 best Val Acc: 33.33%\n",
      " Subject-dependent RLS results: 28.55% ± 6.15%\n",
      "\n",
      "=== CROSS-SUBJECT (RLS) ===\n",
      "Loading SEED-VII dataset from .\n",
      "Found 20 EEG files and 20 eye files\n",
      "Loading subject 1: 1\n",
      "Loading subject 2: 10\n",
      "Loading subject 3: 11\n",
      "Loading subject 4: 12\n",
      "Loading subject 5: 13\n",
      "Loading subject 6: 14\n",
      "Loading subject 7: 15\n",
      "Loading subject 8: 16\n",
      "Loading subject 9: 17\n",
      "Loading subject 10: 18\n",
      "Loading subject 11: 19\n",
      "Loading subject 12: 2\n",
      "Loading subject 13: 20\n",
      "Loading subject 14: 3\n",
      "Loading subject 15: 4\n",
      "Loading subject 16: 5\n",
      "Loading subject 17: 6\n",
      "Loading subject 18: 7\n",
      "Loading subject 19: 8\n",
      "Loading subject 20: 9\n",
      "Dataset loaded: 69742 samples\n",
      "EEG shape: (69742, 310), Eye shape: (69742, 33)\n",
      "Created 1.0% subset: 697 samples\n",
      " TestSubj 0 Epoch 0: Acc 21.2%\n",
      " TestSubj 0 Epoch 10: Acc 21.2%\n",
      " TestSubj 0 Epoch 20: Acc 21.2%\n",
      " TestSubj 0 final Acc: 21.21%\n",
      " TestSubj 1 Epoch 0: Acc 21.9%\n",
      " TestSubj 1 Epoch 10: Acc 12.5%\n",
      " TestSubj 1 Epoch 20: Acc 28.1%\n",
      " TestSubj 1 final Acc: 28.12%\n",
      " TestSubj 2 Epoch 0: Acc 30.2%\n",
      " TestSubj 2 Epoch 10: Acc 14.0%\n",
      " TestSubj 2 Epoch 20: Acc 14.0%\n",
      " TestSubj 2 final Acc: 13.95%\n",
      " Cross-subject RLS results: 21.10% ± 5.79%\n",
      "\n",
      "FINAL RESULTS:\n",
      " subj_dep: 28.55% ± 6.15%\n",
      " cross_subj: 21.10% ± 5.79%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1774b8-c579-4e64-89e0-b6a6677a191a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
