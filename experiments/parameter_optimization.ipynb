{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameter Optimization (WIP)\n",
        "\n",
        "Replace with cells from RLS-ATTN-1p-Parameter-Optimization.ipynb."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef031e40-9c4f-4f3f-af24-793cbd95c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Optimization Analysis for RLS Attention Models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.training.hyperparameter_search import subject_dependent_experiment_with_search\n",
    "from src.training.train_rls import cross_subject_experiment_rls\n",
    "import torch\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75357330-89b0-4b3e-b8c4-2c46431eb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive hyperparameter search\n",
    "DATA_DIR = \".\"\n",
    "SUBSET_RATIO = 0.5  # Use 50% for thorough analysis\n",
    "\n",
    "print(\"=== PARAMETER OPTIMIZATION EXPERIMENTS ===\")\n",
    "\n",
    "# Run hyperparameter search with detailed results\n",
    "avg_acc, std_acc, detailed_results = subject_dependent_experiment_with_search(\n",
    "    DATA_DIR, SUBSET_RATIO\n",
    ")\n",
    "\n",
    "# Load search results for analysis\n",
    "search_results_df = pd.read_csv('hyperparameter_search_results.csv')\n",
    "print(f\"Loaded {len(search_results_df)} hyperparameter combinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52efd791-09d5-439c-a5dd-cfefdea2cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze parameter sensitivity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Lambda regularization analysis\n",
    "lambda_analysis = search_results_df.groupby('lambda_reg')['val_acc'].agg(['mean', 'std']).reset_index()\n",
    "axes[0,0].errorbar(lambda_analysis['lambda_reg'], lambda_analysis['mean'], \n",
    "                   yerr=lambda_analysis['std'], marker='o', capsize=5)\n",
    "axes[0,0].set_xlabel('Lambda Regularization')\n",
    "axes[0,0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[0,0].set_title('Effect of Lambda Regularization')\n",
    "axes[0,0].set_xscale('log')\n",
    "\n",
    "# Forgetting factor analysis\n",
    "ff_analysis = search_results_df.groupby('forgetting_factor')['val_acc'].agg(['mean', 'std']).reset_index()\n",
    "axes[0,1].errorbar(ff_analysis['forgetting_factor'], ff_analysis['mean'], \n",
    "                   yerr=ff_analysis['std'], marker='o', capsize=5)\n",
    "axes[0,1].set_xlabel('Forgetting Factor')\n",
    "axes[0,1].set_ylabel('Validation Accuracy (%)')\n",
    "axes[0,1].set_title('Effect of Forgetting Factor')\n",
    "\n",
    "# Model dimension analysis\n",
    "dim_analysis = search_results_df.groupby('d_model')['val_acc'].agg(['mean', 'std']).reset_index()\n",
    "axes[1,0].errorbar(dim_analysis['d_model'], dim_analysis['mean'], \n",
    "                   yerr=dim_analysis['std'], marker='o', capsize=5)\n",
    "axes[1,0].set_xlabel('Model Dimension')\n",
    "axes[1,0].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1,0].set_title('Effect of Model Dimension')\n",
    "\n",
    "# Learning rate analysis\n",
    "lr_analysis = search_results_df.groupby('learning_rate')['val_acc'].agg(['mean', 'std']).reset_index()\n",
    "axes[1,1].errorbar(lr_analysis['learning_rate'], lr_analysis['mean'], \n",
    "                   yerr=lr_analysis['std'], marker='o', capsize=5)\n",
    "axes[1,1].set_xlabel('Learning Rate')\n",
    "axes[1,1].set_ylabel('Validation Accuracy (%)')\n",
    "axes[1,1].set_title('Effect of Learning Rate')\n",
    "axes[1,1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_sensitivity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9a4da-83b5-4af2-b1e2-6104b3c71628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze best performing parameters\n",
    "print(\"=== BEST PARAMETER COMBINATIONS ===\")\n",
    "\n",
    "# Top 10 parameter combinations\n",
    "top_combinations = search_results_df.nlargest(10, 'val_acc')\n",
    "print(\"\\nTop 10 Parameter Combinations:\")\n",
    "for idx, row in top_combinations.iterrows():\n",
    "    print(f\"Rank {idx+1}: Acc={row['val_acc']:.2f}%, λ={row['lambda_reg']}, \"\n",
    "          f\"ff={row['forgetting_factor']}, dim={row['d_model']}, lr={row['learning_rate']}\")\n",
    "\n",
    "# Parameter frequency in top combinations\n",
    "top_20 = search_results_df.nlargest(20, 'val_acc')\n",
    "param_frequency = {\n",
    "    'lambda_reg': top_20['lambda_reg'].value_counts().to_dict(),\n",
    "    'forgetting_factor': top_20['forgetting_factor'].value_counts().to_dict(),\n",
    "    'd_model': top_20['d_model'].value_counts().to_dict(),\n",
    "    'learning_rate': top_20['learning_rate'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "print(f\"\\nParameter Frequency in Top 20 Combinations:\")\n",
    "for param, freq in param_frequency.items():\n",
    "    print(f\"{param}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b184d-f98c-43b4-bae0-594c289cde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best parameters with cross-subject validation\n",
    "if detailed_results:\n",
    "    best_subject_result = max(detailed_results, key=lambda x: x['test_acc'])\n",
    "    best_overall_params = best_subject_result['best_params']\n",
    "    \n",
    "    print(f\"\\n=== TESTING BEST PARAMETERS ===\")\n",
    "    print(f\"Best parameters from subject {best_subject_result['subject']}: {best_overall_params}\")\n",
    "    \n",
    "    # Run cross-subject with optimized parameters\n",
    "    from src.training.hyperparameter_search import cross_subject_with_best_params\n",
    "    cross_avg, cross_std = cross_subject_with_best_params(DATA_DIR, SUBSET_RATIO, best_overall_params)\n",
    "    \n",
    "    # Compare with default parameters\n",
    "    default_avg, default_std = cross_subject_experiment_rls(DATA_DIR, SUBSET_RATIO)\n",
    "    \n",
    "    print(f\"\\n=== COMPARISON RESULTS ===\")\n",
    "    print(f\"Default Parameters:  {default_avg:.2f}% ± {default_std:.2f}%\")\n",
    "    print(f\"Optimized Parameters: {cross_avg:.2f}% ± {cross_std:.2f}%\")\n",
    "    print(f\"Improvement: {cross_avg - default_avg:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48dfd0f-0ec7-4b61-a0b9-e7e807221bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Subject-dependent vs Cross-subject performance\n",
    "if detailed_results:\n",
    "    subject_accs = [r['test_acc'] for r in detailed_results]\n",
    "    methods = ['Subject-Dependent', 'Cross-Subject (Default)', 'Cross-Subject (Optimized)']\n",
    "    accuracies = [np.mean(subject_accs), default_avg, cross_avg]\n",
    "    errors = [np.std(subject_accs), default_std, cross_std]\n",
    "    \n",
    "    axes[0,0].bar(methods, accuracies, yerr=errors, capsize=5, alpha=0.7)\n",
    "    axes[0,0].set_ylabel('Test Accuracy (%)')\n",
    "    axes[0,0].set_title('Performance Comparison')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Parameter correlation heatmap\n",
    "param_columns = ['lambda_reg', 'forgetting_factor', 'd_model', 'learning_rate', 'val_acc']\n",
    "correlation_matrix = search_results_df[param_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0,1])\n",
    "axes[0,1].set_title('Parameter Correlation Matrix')\n",
    "\n",
    "# Distribution of validation accuracies\n",
    "axes[1,0].hist(search_results_df['val_acc'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1,0].axvline(search_results_df['val_acc'].mean(), color='red', linestyle='--', \n",
    "                  label=f'Mean: {search_results_df[\"val_acc\"].mean():.2f}%')\n",
    "axes[1,0].set_xlabel('Validation Accuracy (%)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "axes[1,0].set_title('Distribution of Validation Accuracies')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Subject-wise performance with best parameters\n",
    "if detailed_results:\n",
    "    subjects = [r['subject'] for r in detailed_results]\n",
    "    test_accs = [r['test_acc'] for r in detailed_results]\n",
    "    axes[1,1].bar(subjects, test_accs, alpha=0.7)\n",
    "    axes[1,1].set_xlabel('Subject ID')\n",
    "    axes[1,1].set_ylabel('Test Accuracy (%)')\n",
    "    axes[1,1].set_title('Subject-wise Performance (Best Parameters)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_results_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88d06a-b51c-4c56-bdd2-29f594c14de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
('hyperparameter_search_results.csv')
print(f"Loaded {len(search_results_df)} hyperparameter combinations")
